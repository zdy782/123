{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analyzing the results for `sciplex_hparam` with `grover` and `rdkit` sweeps\n",
    "\n",
    "This is preliminary to the `fintuning_num_genes` experiments. We look at the results of sweeping the optimisation related hyperparameters for fine-tuning on the sciplex dataset for all other embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import seml\n",
    "from matplotlib import pyplot as plt\n",
    "from chemCPA.paths import FIGURE_DIR\n",
    "\n",
    "matplotlib.style.use(\"fivethirtyeight\")\n",
    "matplotlib.style.use(\"seaborn-talk\")\n",
    "matplotlib.rcParams['font.family'] = \"monospace\"\n",
    "plt.rcParams['savefig.facecolor'] = 'white'\n",
    "sns.set_context(\"poster\")\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = seml.get_results(\n",
    "    \"sciplex_hparam\",\n",
    "    to_data_frame=True,\n",
    "    fields=[\"config\", \"result\", \"seml\", \"config_hash\"],\n",
    "    states=[\"COMPLETED\"],\n",
    "    filter_dict={\n",
    "        # 'batch_id': 3,\n",
    "        'config.dataset.data_params.split_key': 'split_ho_pathway'\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Look at number of experiments per model\n",
    "results[\"config.model.embedding.model\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results.loc[:, [c for c in results.columns if 'pretrain' in c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    results['config.model.embedding.model'],\n",
    "    results['result.perturbation disentanglement'].isnull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in results.columns if 'split' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    results['config.dataset.data_params.split_key'],\n",
    "    results['result.perturbation disentanglement'].isnull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    results['config.dataset.data_params.split_key'],\n",
    "    results['result.loss_reconstruction'].isnull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "results.isnull().any()[results.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows without nans \n",
    "clean_id = results.loc[~results['result.training'].isnull(), '_id']\n",
    "# clean_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweeped_params = [\n",
    "    # \"model.hparams.dim\",\n",
    "    # \"model.hparams.dropout\",\n",
    "    # \"model.hparams.dosers_width\",\n",
    "    # \"model.hparams.dosers_depth\",\n",
    "    \"model.hparams.dosers_lr\",\n",
    "    \"model.hparams.dosers_wd\",\n",
    "    # \"model.hparams.autoencoder_width\",\n",
    "    # \"model.hparams.autoencoder_depth\",\n",
    "    \"model.hparams.autoencoder_lr\",\n",
    "    \"model.hparams.autoencoder_wd\",\n",
    "    \"model.hparams.adversary_width\",\n",
    "    \"model.hparams.adversary_depth\",\n",
    "    \"model.hparams.adversary_lr\",\n",
    "    \"model.hparams.adversary_wd\",\n",
    "    \"model.hparams.adversary_steps\",\n",
    "    \"model.hparams.reg_adversary\",\n",
    "    \"model.hparams.penalty_adversary\",\n",
    "    \"model.hparams.batch_size\",\n",
    "    \"model.hparams.step_size_lr\",\n",
    "    # \"model.hparams.embedding_encoder_width\",\n",
    "    # \"model.hparams.embedding_encoder_depth\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of training runs that resulted in NaNs or totally failed\n",
    "\n",
    "results_clean = results[results._id.isin(clean_id)].copy()\n",
    "print(f\"Percentage of invalid (nan) runs: {1 - len(clean_id) / len(results)}\")\n",
    "\n",
    "# Remove runs with r2 < 0.6 on the training set\n",
    "# results_clean = results_clean[results_clean['result.training'].apply(lambda x: x[0][0])>0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_clean[\"config.model.embedding.model\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate some stats\n",
    "get_mean = lambda x: np.array(x)[-1, 0]\n",
    "get_mean_de = lambda x: np.array(x)[-1, 1]\n",
    "\n",
    "results_clean[\"result.training_mean\"] = results_clean[\"result.training\"].apply(get_mean)\n",
    "results_clean[\"result.training_mean_de\"] = results_clean[\"result.training\"].apply(get_mean_de)\n",
    "results_clean[\"result.val_mean\"] = results_clean[\"result.test\"].apply(get_mean)\n",
    "results_clean[\"result.val_mean_de\"] = results_clean[\"result.test\"].apply(get_mean_de)\n",
    "results_clean[\"result.test_mean\"] = results_clean[\"result.ood\"].apply(get_mean)\n",
    "results_clean[\"result.test_mean_de\"] = results_clean[\"result.ood\"].apply(get_mean_de)\n",
    "results_clean[\"result.perturbation disentanglement\"] = results_clean[\"result.perturbation disentanglement\"].apply(lambda x: x[0])\n",
    "results_clean[\"result.covariate disentanglement\"] = results_clean[\"result.covariate disentanglement\"].apply(lambda x: x[0][0])\n",
    "results_clean[\"result.final_reconstruction\"] = results_clean[\"result.loss_reconstruction\"].apply(lambda x: x[-1])\n",
    "\n",
    "results_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_clean[\"config.model.load_pretrained\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(data=results_clean[\"result.epoch\"].apply(max))\n",
    "ax.set_title(\"Total epochs before final stopping (min 125)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at $r^2$ reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in results_clean.columns if 'pretrain' in c]\n",
    "\n",
    "results_clean[[\"config.model.embedding.model\", 'config.model.load_pretrained']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DE genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE genes\n",
    "rows, cols = 1, 3\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(10*cols,6*rows))\n",
    "\n",
    "for i, y in enumerate((\"result.training_mean_de\", \"result.val_mean_de\", \"result.test_mean_de\")):\n",
    "    sns.violinplot(\n",
    "        data=results_clean, \n",
    "        x=\"config.model.embedding.model\", \n",
    "        y=y, \n",
    "        hue='config.model.load_pretrained', \n",
    "        inner='points', \n",
    "        ax=ax[i], \n",
    "        scale='width',\n",
    "        )\n",
    "    ax[i].set_ylim([0.3,1.01])\n",
    "    ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=75, ha='right')\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].set_ylabel(y.split('.')[-1])\n",
    "    ax[i].legend(title='Pretrained',loc='lower right', fontsize=18, title_fontsize=24)\n",
    "\n",
    "ax[0].get_legend().remove()\n",
    "ax[1].get_legend().remove()\n",
    "ax[2].legend(title='Pretrained', fontsize=18, title_fontsize=24, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE genes\n",
    "rows, cols = 1, 3\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(10*cols,6*rows))\n",
    "\n",
    "for i, y in enumerate((\"result.training_mean\", \"result.val_mean\", \"result.test_mean\")):\n",
    "    sns.violinplot(\n",
    "        data=results_clean, \n",
    "        x=\"config.model.embedding.model\", \n",
    "        y=y, \n",
    "        hue='config.model.load_pretrained', \n",
    "        inner='points', \n",
    "        ax=ax[i], \n",
    "        scale='width',\n",
    "        )\n",
    "    ax[i].set_ylim([0.3,1.01])\n",
    "    ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=75, ha='right')\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].set_ylabel(y.split('.')[-1])\n",
    "    ax[i].legend(title='Pretrained',loc='lower right', fontsize=18, title_fontsize=24)\n",
    "\n",
    "ax[0].get_legend().remove()\n",
    "ax[1].get_legend().remove()\n",
    "ax[2].legend(title='Pretrained', fontsize=18, title_fontsize=24, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at disentanglement scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 2 \n",
    "cols = 1\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(10*cols, 7*rows), sharex=True)\n",
    "\n",
    "max_entangle = [0.07, 0.65]\n",
    "for i, y in enumerate([\"result.perturbation disentanglement\", \"result.covariate disentanglement\"]):\n",
    "    sns.violinplot(data=results_clean, x=\"config.model.embedding.model\", y=y, inner='point' ,ax=ax[i], hue='config.model.load_pretrained')\n",
    "    # ax[i].set_ylim([0,1])\n",
    "    x_ticks = ax[i].get_xticklabels()\n",
    "    [x_tick.set_text(x_tick.get_text().split(\"_\")[0]) for x_tick in x_ticks]\n",
    "    ax[i].set_xticklabels(x_ticks, rotation=25, ha='center')\n",
    "    ax[i].axhline(max_entangle[i],ls=\":\", color='black')\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].set_ylabel(y.split('.')[-1])\n",
    "ax[1].get_legend().remove()\n",
    "ax[0].legend(title='Pretrained', fontsize=18, title_fontsize=24, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subselect to disentangled models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 2\n",
    "\n",
    "def performance_condition (emb, pretrained, max_entangle, max_entangle_cov): \n",
    "    cond = (results_clean[\"config.model.embedding.model\"] == emb) \n",
    "    cond = cond & (results_clean[\"result.perturbation disentanglement\"] < max_entangle) \n",
    "    cond = cond & (results_clean[\"config.model.load_pretrained\"] == pretrained)\n",
    "    cond = cond & (results_clean[\"result.covariate disentanglement\"] < max_entangle_cov)\n",
    "    return cond \n",
    "\n",
    "best = []\n",
    "for embedding in list(results_clean[\"config.model.embedding.model\"].unique()):\n",
    "    for pretrained in [True, False]:\n",
    "        df = results_clean[performance_condition(embedding, pretrained, max_entangle[0], max_entangle[1])]\n",
    "        print(embedding, pretrained, len(df))\n",
    "        # if len(df) == 0: \n",
    "        #     df = results_clean[performance_condition(embedding, pretrained, max_entangle[0], max_entangle[1]+0.05)]\n",
    "        #     if len(df) == 0: \n",
    "        #         df = results_clean[performance_condition(embedding, pretrained, max_entangle[0], max_entangle[1]+0.2)]\n",
    "        #         if len(df) == 0: \n",
    "        #             df = results_clean[performance_condition(embedding, pretrained, max_entangle[0], max_entangle[1]+0.3)]\n",
    "        if not pretrained and len(df) == 0: \n",
    "            best = best[:-1]   # delete previous run\n",
    "        best.append(df.sort_values(by=\"result.val_mean_de\", ascending=False).head(n_top))\n",
    "\n",
    "best = pd.concat(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All genes, DE genes, disentanglement\n",
    "rows, cols = 2, 2\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(10*cols,6*rows))\n",
    "\n",
    "for i, y in enumerate([\"result.test_mean\", \"result.test_mean_de\", \"result.perturbation disentanglement\", \"result.covariate disentanglement\"]):\n",
    "    sns.violinplot(data=best, x=\"config.model.embedding.model\", y=y, inner='points', ax=ax[i//cols, i%cols], scale='area', hue='config.model.load_pretrained')\n",
    "    x_ticks = ax[i//cols, i%cols].get_xticklabels()\n",
    "    [x_tick.set_text(x_tick.get_text().split(\"_\")[0]) for x_tick in x_ticks]\n",
    "    ax[i//cols, i%cols].set_xticklabels(x_ticks, rotation=25, ha='center')\n",
    "    ax[i//cols, i%cols].set_xlabel('')\n",
    "    ax[i//cols, i%cols].set_ylabel(y.split('.')[-1])\n",
    "ax[0,0].set_ylabel(\"$\\mathbb{E}\\,[R^2]$ on all genes\")\n",
    "# ax[0,0].set_ylim([0.89, 0.96])\n",
    "ax[0,1].set_ylabel(\"$\\mathbb{E}\\,[R^2]$ on DE genes\")\n",
    "ax[0,1].set_ylim([0.59,0.91])\n",
    "\n",
    "ax[1,0].set_ylabel(\"Drug entanglement\")\n",
    "ax[1,0].axhline(max_entangle[0],ls=':' ,color='black')\n",
    "ax[1,0].text(3.0, max_entangle[0]+0.003, 'max entangled', fontsize=15, va='center', ha='center')\n",
    "ax[1,0].set_ylim([-0.01, 0.089])\n",
    "ax[1,1].set_ylabel(\"Covariate entanglement\")\n",
    "ax[1,1].text(3.0, max_entangle[1]+0.015, 'max entangled', fontsize=15, va='center', ha='center')\n",
    "ax[1,1].axhline(max_entangle[1], ls=':', color='black')\n",
    "\n",
    "ax[0,0].get_legend().remove()\n",
    "ax[1,0].get_legend().remove()\n",
    "ax[1,1].get_legend().remove()\n",
    "ax[0,1].legend(title='Pretrained', fontsize=18, title_fontsize=24, loc=\"center left\", bbox_to_anchor=(1, 0.6))\n",
    "plt.tight_layout()\n",
    "\n",
    "split_keys = results_clean['config.dataset.data_params.split_key'].unique()\n",
    "assert len(split_keys) == 1 \n",
    "split_key = split_keys[0]\n",
    "\n",
    "plt.savefig(FIGURE_DIR/f'sciplex_{split_key}_lincs_genes.eps', format='eps', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 1, 3\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(10*cols,6*rows))\n",
    "\n",
    "for i, y in enumerate([\"result.training_mean\", \"result.training_mean_de\", \"result.perturbation disentanglement\"]):\n",
    "    sns.violinplot(\n",
    "        data=best, \n",
    "        x=\"config.model.embedding.model\", \n",
    "        y=y, \n",
    "        hue='config.model.load_pretrained', \n",
    "        inner='points', \n",
    "        ax=ax[i], \n",
    "        scale='width',\n",
    "        )\n",
    "    ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=75, ha='right')\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].set_ylabel(y.split('.')[-1])\n",
    "ax[0].get_legend().remove()\n",
    "ax[0].set_ylim([0.4, 1.01])\n",
    "ax[1].get_legend().remove()\n",
    "ax[1].set_ylim([0.4, 1.01])\n",
    "ax[2].legend(title='Pretrained', fontsize=18, title_fontsize=24, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a deeper look in the `.config` of the best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best[[\"config.\" + col for col in sweeped_params] +\n",
    "     [\"result.perturbation disentanglement\", \"result.test_mean\", \"result.test_mean_de\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
