{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analyzing the results for `sciplex_hparam`\n",
    "\n",
    "This is preliminary to the `fintuning_num_genes` experiments. We look at the results of sweeping the optimisation related hyperparameters for fine-tuning on the sciplex dataset for all other embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import seml\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "matplotlib.style.use(\"fivethirtyeight\")\n",
    "matplotlib.style.use(\"seaborn-talk\")\n",
    "matplotlib.rcParams['font.family'] = \"monospace\"\n",
    "plt.rcParams['savefig.facecolor'] = 'white'\n",
    "sns.set_context(\"poster\")\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = seml.get_results(\n",
    "    \"sciplex_hparam\",\n",
    "    to_data_frame=True,\n",
    "    fields=[\"config\", \"result\", \"seml\", \"config_hash\"],\n",
    "    states=[\"COMPLETED\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Look at number of experiments per model\n",
    "results[\"config.model.embedding.model\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results.loc[:, [c for c in results.columns if 'disentanglement' in c]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweeped_params = [\n",
    "    # \"model.hparams.dim\",\n",
    "    # \"model.hparams.dropout\",\n",
    "    # \"model.hparams.dosers_width\",\n",
    "    # \"model.hparams.dosers_depth\",\n",
    "    \"model.hparams.dosers_lr\",\n",
    "    \"model.hparams.dosers_wd\",\n",
    "    # \"model.hparams.autoencoder_width\",\n",
    "    # \"model.hparams.autoencoder_depth\",\n",
    "    \"model.hparams.autoencoder_lr\",\n",
    "    \"model.hparams.autoencoder_wd\",\n",
    "    \"model.hparams.adversary_width\",\n",
    "    \"model.hparams.adversary_depth\",\n",
    "    \"model.hparams.adversary_lr\",\n",
    "    \"model.hparams.adversary_wd\",\n",
    "    \"model.hparams.adversary_steps\",\n",
    "    \"model.hparams.reg_adversary\",\n",
    "    \"model.hparams.penalty_adversary\",\n",
    "    \"model.hparams.batch_size\",\n",
    "    \"model.hparams.step_size_lr\",\n",
    "    # \"model.hparams.embedding_encoder_width\",\n",
    "    # \"model.hparams.embedding_encoder_depth\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of training runs that resulted in NaNs or totally failed\n",
    "nan_results = results[results[\"result.loss_reconstruction\"].apply(lambda x: math.isnan(sum(x)))]\n",
    "results_clean = results[~results[\"result.loss_reconstruction\"].apply(lambda x: math.isnan(sum(x)))].copy()\n",
    "print(len(nan_results) / len(results))\n",
    "\n",
    "# Remove runs with r2 < 0.6 on the training set\n",
    "results_clean = results_clean[results_clean['result.training'].apply(lambda x: x[0][0])>0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_clean[\"config.model.embedding.model\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate some stats\n",
    "get_mean = lambda x: np.array(x)[-1, 0]\n",
    "get_mean_de = lambda x: np.array(x)[-1, 1]\n",
    "\n",
    "results_clean[\"result.training_mean\"] = results_clean[\"result.training\"].apply(get_mean)\n",
    "results_clean[\"result.training_mean_de\"] = results_clean[\"result.training\"].apply(get_mean_de)\n",
    "results_clean[\"result.val_mean\"] = results_clean[\"result.test\"].apply(get_mean)\n",
    "results_clean[\"result.val_mean_de\"] = results_clean[\"result.test\"].apply(get_mean_de)\n",
    "results_clean[\"result.test_mean\"] = results_clean[\"result.ood\"].apply(get_mean)\n",
    "results_clean[\"result.test_mean_de\"] = results_clean[\"result.ood\"].apply(get_mean_de)\n",
    "results_clean[\"result.perturbation disentanglement\"] = results_clean[\"result.perturbation disentanglement\"].apply(lambda x: x[0])\n",
    "\n",
    "\n",
    "results_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(data=results_clean[\"result.epoch\"].apply(max))\n",
    "ax.set_title(\"Total epochs before final stopping (min 125)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at $r^2$ reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in results_clean.columns if 'pretrain' in c]\n",
    "\n",
    "results_clean[[\"config.model.embedding.model\", 'config.model.load_pretrained']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DE genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 1 \n",
    "cols = 3\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(7*cols, 7*rows), sharex=True)\n",
    "\n",
    "for i, y in enumerate((\"result.training_mean_de\", \"result.val_mean_de\", \"result.test_mean_de\")):\n",
    "    sns.violinplot(data=results_clean, x=\"config.model.embedding.model\", y=y, inner='point' ,ax=ax[i], scale='width')\n",
    "    ax[i].set_ylim([0.0,1])\n",
    "    ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=75, ha=\"right\")\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].set_ylabel(y.split('.')[-1])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 1 \n",
    "cols = 3\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(10*cols, 7*rows), sharex=True)\n",
    "\n",
    "for i, y in enumerate((\"result.training_mean\", \"result.val_mean\", \"result.test_mean\")):\n",
    "    # sns.violinplot(data=results_clean[results_clean['config.model.load_pretrained']==True], x=\"config.model.embedding.model\", y=y, inner='point' ,ax=ax[i], scale='width')\n",
    "    sns.violinplot(\n",
    "        data=results_clean,\n",
    "        x=\"config.model.embedding.model\", \n",
    "        y=y, \n",
    "        # hue='config.model.load_pretrained',\n",
    "        inner='point',\n",
    "        ax=ax[i], \n",
    "        scale='width')\n",
    "    # ax[i].set_ylim([0.82,1])\n",
    "    ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=75, ha='right')\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].set_ylabel(y.split('.')[-1])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at disentanglement scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 1 \n",
    "cols = 1\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(10*cols, 7*rows), sharex=True)\n",
    "\n",
    "for y in [\"result.perturbation disentanglement\"]:\n",
    "    sns.violinplot(data=results_clean, x=\"config.model.embedding.model\", y=y, inner='point' ,ax=ax, scale='width')\n",
    "    # ax[i].set_ylim([0,1])\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=75, ha='right')\n",
    "    ax.axhline(0.18, color='orange')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel(y.split('.')[-1])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subselect to disentangled models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 5\n",
    "\n",
    "performance_condition = lambda emb, pretrained, max_entangle: (results_clean[\"config.model.embedding.model\"] == emb) & (results_clean[\"result.perturbation disentanglement\"] < max_entangle) & (results_clean[\"config.model.load_pretrained\"] == pretrained)\n",
    "\n",
    "best = []\n",
    "for embedding in list(results_clean[\"config.model.embedding.model\"].unique()):\n",
    "    for pretrained in [True, False]:\n",
    "        df = results_clean[performance_condition(embedding, pretrained, 0.18)]\n",
    "        print(embedding, pretrained, len(df))\n",
    "        best.append(df.sort_values(by=\"result.val_mean_de\", ascending=False).head(n_top))\n",
    "\n",
    "best = pd.concat(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check the disentanglement computation \n",
    "2. Plot the UMAP on the drug latens and compare from scratch vs. pre-trained\n",
    "    * This would be a major selling point\n",
    "3. Other metics but R2? CPA Review: Wasserstein distance? \n",
    "4. Better variance r2 for pre-trained models? Variance of the gaussian output (3rd and 4th output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All genes, DE genes, disentanglement\n",
    "rows, cols = 1, 3\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(10*cols,6*rows))\n",
    "\n",
    "for i, y in enumerate([\"result.test_mean\", \"result.test_mean_de\", \"result.perturbation disentanglement\"]):\n",
    "    sns.violinplot(\n",
    "        data=best, \n",
    "        x=\"config.model.embedding.model\", \n",
    "        y=y, \n",
    "        hue='config.model.load_pretrained', \n",
    "        inner='points', \n",
    "        ax=ax[i], \n",
    "        scale='width',\n",
    "        )\n",
    "    # ax[i].set_ylim([0.75, 1.01])\n",
    "    ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=75, ha='right')\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].set_ylabel(y.split('.')[-1])\n",
    "    ax[i].legend(title='Pretrained',loc='lower right', fontsize=18, title_fontsize=24)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 1, 3\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(10*cols,6*rows))\n",
    "\n",
    "for i, y in enumerate([\"result.training_mean\", \"result.training_mean_de\", \"result.perturbation disentanglement\"]):\n",
    "    sns.violinplot(\n",
    "        data=best, \n",
    "        x=\"config.model.embedding.model\", \n",
    "        y=y, \n",
    "        hue='config.model.load_pretrained', \n",
    "        inner='points', \n",
    "        ax=ax[i], \n",
    "        scale='width',\n",
    "        )\n",
    "    ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=75, ha='right')\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].set_ylabel(y.split('.')[-1])\n",
    "    ax[i].legend(title='Pretrained',loc='best', fontsize=18, title_fontsize=24)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a deeper look in the `.config` of the best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best[[\"config.\" + col for col in sweeped_params] +\n",
    "     [\"result.perturbation disentanglement\", \"result.test_mean\", \"result.test_mean_de\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
